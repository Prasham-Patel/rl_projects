{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install -U colabgymrender"
      ],
      "metadata": {
        "id": "1s6Z6lAN0Yvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow.python.keras as keras\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "\n",
        "class ActorCriticNetwork(keras.Model):\n",
        "    def __init__(self, actions, Layers = [1024, 512], name = \"actor-critic\", chkpt_dir = \"tmp/actor_critic\"):\n",
        "        super(ActorCriticNetwork, self).__init__()\n",
        "        self.Layer1_dim = Layers[0]\n",
        "        self.Layer2_dim = Layers[1]\n",
        "        self.actions = actions\n",
        "        self.model_name = name\n",
        "        self.checkpoint_dir = chkpt_dir\n",
        "        self.file = os.path.join(self.checkpoint_dir, name+\"ac\")\n",
        "\n",
        "        self.Layer1 = Dense(self.Layer1_dim, activation= 'relu')\n",
        "        self.Layer2 = Dense(self.Layer2_dim, activation= 'relu')\n",
        "        self.value = Dense(1, activation= None)\n",
        "        self.policy = Dense(actions, activation=\"softmax\")\n",
        "\n",
        "    def call(self, state):\n",
        "        value = self.Layer1(state)\n",
        "        value = self.Layer2(value)\n",
        "        v = self.value(value)\n",
        "        pi = self.policy(value)\n",
        "        return v, pi"
      ],
      "metadata": {
        "id": "O5-lSpml3RXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import float32\n",
        "import tensorflow as tf\n",
        "import tensorflow.python.keras as keras\n",
        "import keras\n",
        "# from keras.optimizers import Adam\n",
        "import tensorflow_probability as tfp\n",
        "# from networks import ActorCriticNetwork\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, alpha = 0.0003, gamma = 0.99, num_actions = 2):\n",
        "        self.gamma = gamma\n",
        "        self.num_actions = num_actions\n",
        "        self.action = None\n",
        "        self.action_space = [i for i in range(self.num_actions)]\n",
        "        self.actor_critic = ActorCriticNetwork(num_actions)\n",
        "        # opt = keras.optimizers.Adam(learning_rate=alpha)\n",
        "        self.actor_critic.compile(optimizer= 'adam')\n",
        "\n",
        "    def choose_action(self, observation):\n",
        "        # print([observation].shape)\n",
        "\n",
        "        state = tf.convert_to_tensor(np.asarray([observation], dtype=float32))\n",
        "        _, probs = self.actor_critic(state)\n",
        "        action_probability = tfp.distributions.Categorical(probs)\n",
        "        actions = action_probability.sample()\n",
        "        self.actions = actions\n",
        "\n",
        "        return self.actions.numpy()[0]\n",
        "\n",
        "    def save_model(self):\n",
        "        print(\"....saving model....\")\n",
        "        self.actor_critic.save_weights(self.actor_critic.checkpoint_dir)\n",
        "\n",
        "    def load_model(self):\n",
        "        print(\"...loading model...\")\n",
        "        self.actor_critic.load_weights(self.actor_critic.checkpoint_dir)\n",
        "\n",
        "    def learn(self, state, reward, state_next, done):\n",
        "        state = tf.convert_to_tensor([state], dtype=float32)\n",
        "        state_next = tf.convert_to_tensor([state_next], dtype=float32)\n",
        "        reward = tf.convert_to_tensor([reward], dtype=float32)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            state_value, probs = self.actor_critic(state)\n",
        "            state_value_next, _ = self.actor_critic(state_next)\n",
        "            state_value = tf.squeeze(state_value)\n",
        "            state_value_next = tf.squeeze(state_value_next)\n",
        "\n",
        "            action_probs = tfp.distributions.Categorical(probs)\n",
        "            log_prob = action_probs.log_prob(self.actions)\n",
        "\n",
        "            delta = reward + self.gamma*state_value_next*(1 - int(done)) - state_value\n",
        "            actor_loss = -log_prob * delta\n",
        "            critic_loss = delta**2\n",
        "\n",
        "            total_loss = actor_loss + critic_loss\n",
        "\n",
        "        gradient = tape.gradient(total_loss, self.actor_critic.trainable_variables)\n",
        "        self.actor_critic.optimizer.apply_gradients(zip(gradient, self.actor_critic.trainable_variables))\n",
        "\n"
      ],
      "metadata": {
        "id": "oxzsmXBJ31dp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "# from actor_critic import Agent\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
        "agent = Agent(alpha=1e-3, num_actions=2)\n",
        "num_games = 2000\n",
        "\n",
        "filename = \"cartpole.png\"\n",
        "figure_file = 'plots/' + filename\n",
        "score_history = []\n",
        "load_checkpoint = False\n",
        "best_score = 0\n",
        "\n",
        "if load_checkpoint:\n",
        "    agent.load_model()\n",
        "\n",
        "# env = Recorder(env, \"cartpole\")\n",
        "\n",
        "\n",
        "for i in range(num_games):\n",
        "    print(\"game num \", i)\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "    while not done:\n",
        "        action = agent.choose_action(observation)\n",
        "        (observation_next, reward, done, info) = env.step(action)\n",
        "        score += reward\n",
        "        if not load_checkpoint:\n",
        "            # print(observation)\n",
        "            agent.learn(observation, reward, observation_next, done)\n",
        "            observation = observation_next\n",
        "    score_history.append(score)\n",
        "    # avg_score = np.mean(score_history[-100:])\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        if not load_checkpoint:\n",
        "            agent.save_model()\n",
        "\n",
        "x = [i+1 for i in range(num_games)]\n",
        "plt.plot(score_history)"
      ],
      "metadata": {
        "id": "WxRWKywl0XoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Recorder(env, \"cartpole\")\n",
        "observation = env.reset()\n",
        "done = False\n",
        "score = 0\n",
        "while not done:\n",
        "    action = agent.choose_action(observation)\n",
        "    (observation_next, reward, done, info) = env.step(action)\n",
        "    score += reward\n",
        "    if not load_checkpoint:\n",
        "        # print(observation)\n",
        "        agent.learn(observation, reward, observation_next, done)\n",
        "        observation = observation_next"
      ],
      "metadata": {
        "id": "oryUMkZq1u34"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}